{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b06c8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-25T15:36:45.274531Z",
     "iopub.status.busy": "2025-06-25T15:36:45.274166Z",
     "iopub.status.idle": "2025-06-25T15:38:54.925838Z",
     "shell.execute_reply": "2025-06-25T15:38:54.924885Z"
    },
    "papermill": {
     "duration": 129.65901,
     "end_time": "2025-06-25T15:38:54.928135",
     "exception": false,
     "start_time": "2025-06-25T15:36:45.269125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#This is for removing kaggle errors\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Don't use other value than GPU 0, code crashes on kaggle notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import periodogram\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU config error: {e}\")\n",
    "except ImportError as e:\n",
    "    print(f\"TensorFlow import error: {e}\")\n",
    "    raise\n",
    "\n",
    "#Data Loading \n",
    "def find_largest_json_files(directory, n=3): #You can change value of n to include more files\n",
    "    try:\n",
    "        files = []\n",
    "        if not os.path.exists(directory):\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory}\")      \n",
    "        for f in Path(directory).rglob('*.json'):\n",
    "            try:\n",
    "                size = os.path.getsize(f)\n",
    "                files.append((str(f), size))\n",
    "            except (OSError, PermissionError) as e:\n",
    "                print(f\"Skipping file {f}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No JSON files found in {directory}\")\n",
    "        \n",
    "        files.sort(key=lambda x: -x[1])\n",
    "        return [f[0] for f in files[:n]]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error finding files in {directory}: {str(e)}\")\n",
    "\n",
    "def parse_transaction_data(file_paths):\n",
    "    records = []\n",
    "    for file in file_paths:\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for tx_type, txs in data.items():\n",
    "                    if not isinstance(txs, list):\n",
    "                        txs = txs.get('transactions', [])\n",
    "                    for tx in txs:\n",
    "                        try:\n",
    "                            wallet = tx.get('account', {}).get('id') or tx.get('liquidatee', {}).get('id')\n",
    "                            if not wallet:\n",
    "                                continue\n",
    "                            \n",
    "                            timestamp = tx.get('timestamp')\n",
    "                            if isinstance(timestamp, str):\n",
    "                                try:\n",
    "                                    timestamp = int(timestamp)\n",
    "                                except ValueError:\n",
    "                                    timestamp = 0\n",
    "                            elif not isinstance(timestamp, (int, float)):\n",
    "                                timestamp = 0\n",
    "                            \n",
    "                            records.append({\n",
    "                                'wallet': wallet.lower(),\n",
    "                                'action': tx_type,\n",
    "                                'amountUSD': float(tx.get('amountUSD', 0)),\n",
    "                                'timestamp': timestamp,\n",
    "                                'hash': tx.get('hash', '') \n",
    "                            })\n",
    "                        except (AttributeError, ValueError, TypeError) as e:\n",
    "                            print(f\"Skipping malformed transaction in {file}: {str(e)}\")\n",
    "                            continue\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            print(f\"Skipping invalid file {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not records:\n",
    "        raise ValueError(\"No valid transactions found in any files\")\n",
    "    \n",
    "    df = pd.DataFrame(records).drop_duplicates(subset=['wallet', 'hash'], keep='first')\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "    \n",
    "    invalid_timestamps = df['datetime'].isna()\n",
    "    if invalid_timestamps.any():\n",
    "        print(f\"Warning: {invalid_timestamps.sum()} transactions had invalid timestamps\")\n",
    "        median_timestamp = df.loc[~invalid_timestamps, 'datetime'].median()\n",
    "        df.loc[invalid_timestamps, 'datetime'] = median_timestamp\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Feature Engineering\n",
    "def calculate_temporal_features(wallet_transactions):\n",
    "    default_values = {\n",
    "        'time_entropy': 0,\n",
    "        'periodicity': 0,\n",
    "        'time_skew': 0,\n",
    "        'time_kurtosis': 0,\n",
    "        'autocorr_1d': 0,\n",
    "        'autocorr_7d': 0,\n",
    "        'active_hours': 0,\n",
    "        'time_std': 0,\n",
    "        'recency': 0\n",
    "    }\n",
    "    \n",
    "    if len(wallet_transactions) < 1:\n",
    "        return default_values\n",
    "    \n",
    "    try:\n",
    "        times = wallet_transactions['datetime'].sort_values()\n",
    "        \n",
    "        last_activity = times.max()\n",
    "        current_time = pd.Timestamp.now()\n",
    "        recency_days = (current_time - last_activity).total_seconds() / 86400\n",
    "        default_values['recency'] = recency_days\n",
    "        \n",
    "        if len(wallet_transactions) < 2:\n",
    "            return default_values\n",
    "            \n",
    "        time_diffs = times.diff().dt.total_seconds().dropna()\n",
    "        \n",
    "        if len(time_diffs) > 0:\n",
    "            default_values.update({\n",
    "                'time_skew': skew(time_diffs),\n",
    "                'time_kurtosis': kurtosis(time_diffs),\n",
    "                'time_std': np.log1p(time_diffs.std())\n",
    "            })\n",
    "        \n",
    "        try:\n",
    "            if len(time_diffs) > 1:\n",
    "                f, Pxx = periodogram(time_diffs, detrend='linear')\n",
    "                default_values['periodicity'] = np.sum(Pxx[f < 1e-3])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "        hours = times.dt.hour\n",
    "        hour_counts = hours.value_counts(normalize=True)\n",
    "        if len(hour_counts) > 0:\n",
    "            default_values.update({\n",
    "                'active_hours': len(hour_counts[hour_counts > 0.05]),\n",
    "                'time_entropy': -np.sum(hour_counts * np.log(hour_counts + 1e-10))\n",
    "            })\n",
    "        \n",
    "        try:\n",
    "            if len(time_diffs) > 7:\n",
    "                acf_vals = acf(time_diffs, nlags=7, fft=True)\n",
    "                default_values.update({\n",
    "                    'autocorr_1d': acf_vals[1],\n",
    "                    'autocorr_7d': acf_vals[-1]\n",
    "                })\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return default_values\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating temporal features: {str(e)}\")\n",
    "        return default_values\n",
    "\n",
    "def create_wallet_features(transactions):\n",
    "    if transactions.empty:\n",
    "        raise ValueError(\"Empty transaction dataframe provided\")\n",
    "    if 'wallet' not in transactions.columns:\n",
    "        raise ValueError(\"Transactions dataframe missing 'wallet' column\")\n",
    "    try:\n",
    "        action_counts = pd.pivot_table(\n",
    "            transactions,\n",
    "            index='wallet',\n",
    "            columns='action',\n",
    "            values='amountUSD',\n",
    "            aggfunc='count',\n",
    "            fill_value=0\n",
    "        )\n",
    "        action_counts.columns = [f\"count_{col}\" for col in action_counts.columns]\n",
    "        \n",
    "        financial = transactions.groupby('wallet').agg({\n",
    "            'amountUSD': ['sum', 'mean', 'std', 'max', 'min', 'count', \n",
    "                         lambda x: np.median(np.abs(x - np.mean(x)))],\n",
    "            'timestamp': [lambda x: (x.max() - x.min())/86400 if len(x) > 1 else 0]\n",
    "        })\n",
    "        financial.columns = [\n",
    "            'total_volume', 'avg_amount', 'amount_std',\n",
    "            'max_amount', 'min_amount', 'txn_count', 'amount_mad', 'activity_days'\n",
    "        ]\n",
    "        \n",
    "        risk_metrics = transactions.groupby('wallet').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'liquidation_ratio': x[x['action'] == 'liquidates'].shape[0] / x.shape[0] if x.shape[0] > 0 else 0,\n",
    "                'large_tx_ratio': x[x['amountUSD'] > 10 * x['amountUSD'].mean()].shape[0] / x.shape[0] if x.shape[0] > 0 else 0,\n",
    "                'volatility': x['amountUSD'].std() / (x['amountUSD'].mean() + 1e-10)\n",
    "            })\n",
    "        ).fillna(0)\n",
    "\n",
    "        temporal_features = transactions.groupby('wallet').apply(\n",
    "            lambda x: pd.Series(calculate_temporal_features(x)))\n",
    "        \n",
    "        required_temporal_cols = [\n",
    "            'time_entropy', 'periodicity', 'time_skew', 'time_kurtosis',\n",
    "            'autocorr_1d', 'autocorr_7d', 'active_hours', 'time_std', 'recency'\n",
    "        ]\n",
    "        for col in required_temporal_cols:\n",
    "            if col not in temporal_features.columns:\n",
    "                temporal_features[col] = 0\n",
    "        \n",
    "        all_indices = [\n",
    "            action_counts.index,\n",
    "            financial.index,\n",
    "            risk_metrics.index,\n",
    "            temporal_features.index\n",
    "        ]\n",
    "        common_index = all_indices[0]\n",
    "        for idx in all_indices[1:]:\n",
    "            common_index = common_index.intersection(idx)\n",
    "        \n",
    "        action_counts = action_counts.reindex(common_index, fill_value=0)\n",
    "        financial = financial.reindex(common_index, fill_value=0)\n",
    "        risk_metrics = risk_metrics.reindex(common_index, fill_value=0)\n",
    "        temporal_features = temporal_features.reindex(common_index, fill_value=0)\n",
    "        \n",
    "        features = pd.concat([\n",
    "            action_counts, \n",
    "            financial, \n",
    "            risk_metrics, \n",
    "            temporal_features\n",
    "        ], axis=1).fillna(0)\n",
    "\n",
    "        features['stability'] = 1 / (1 + features['amount_std'])\n",
    "        features['diversity'] = (action_counts > 0).sum(axis=1) / len(action_counts.columns)\n",
    "        features['time_variability'] = features[['time_skew', 'time_kurtosis', 'time_std']].mean(axis=1)\n",
    "        features['behavior_consistency'] = features[['autocorr_1d', 'autocorr_7d']].mean(axis=1)\n",
    "        features['recency_score'] = np.exp(-features['recency'] / 30) \n",
    "        \n",
    "        features['risk_profile'] = (\n",
    "            0.4 * features['liquidation_ratio'] + \n",
    "            0.3 * features['volatility'] + \n",
    "            0.2 * features['large_tx_ratio'] + \n",
    "            0.1 * (1 - features['behavior_consistency'])\n",
    "        )\n",
    "        \n",
    "        features.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in feature engineering: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "def compute_credit_scores(features):\n",
    "    if len(features) == 0:\n",
    "        raise ValueError(\"Empty features DataFrame\")\n",
    "    \n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(features)\n",
    "        \n",
    "         # Neural Networks Clustering\n",
    "        def create_autoencoder(input_dim, encoding_dim=32):\n",
    "            input_layer = Input(shape=(input_dim,))\n",
    "            encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "            decoder = Dense(input_dim, activation='linear')(encoder)\n",
    "            autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "            encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
    "            return autoencoder, encoder_model\n",
    "\n",
    "        # Training of autoencoder\n",
    "        autoencoder, encoder = create_autoencoder(X.shape[1])\n",
    "        autoencoder.compile(optimizer=Adam(0.001), loss='mse')\n",
    "        autoencoder.fit(X, X, epochs=100, batch_size=256, verbose=0)\n",
    "        \n",
    "        encoded = encoder.predict(X)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "        clusters = kmeans.fit_predict(encoded)\n",
    "        \n",
    "        lof = LocalOutlierFactor(\n",
    "            n_neighbors=min(20, len(X)-1),\n",
    "            contamination=0.1,\n",
    "            novelty=False\n",
    "        )\n",
    "        anomalies = lof.fit_predict(X)\n",
    "        \n",
    "        stability = features['stability'].values\n",
    "        diversity = features['diversity'].values\n",
    "        volume = np.log1p(features['total_volume'].values)\n",
    "        risk = features['liquidation_ratio'].values\n",
    "        \n",
    "        behavior_score = (\n",
    "            0.4 * (volume / np.max(volume)) +\n",
    "            0.3 * stability +\n",
    "            0.2 * diversity +\n",
    "            0.1 * (1 - risk)\n",
    "        )\n",
    "        \n",
    "        percentiles = np.percentile(behavior_score, [0, 50, 75, 90, 95, 98, 100])\n",
    "        scores = np.zeros_like(behavior_score)\n",
    "        \n",
    "        masks = [\n",
    "            behavior_score >= percentiles[5],\n",
    "            (behavior_score >= percentiles[4]) & (behavior_score < percentiles[5]),\n",
    "            (behavior_score >= percentiles[3]) & (behavior_score < percentiles[4]),\n",
    "            (behavior_score >= percentiles[2]) & (behavior_score < percentiles[3]),\n",
    "            (behavior_score >= percentiles[1]) & (behavior_score < percentiles[2]),\n",
    "            behavior_score < percentiles[1]\n",
    "        ]\n",
    "        \n",
    "        ranges = [\n",
    "            (90, 101), (80, 90), (70, 80), (60, 70), (40, 60), (0, 40)\n",
    "        ]\n",
    "        \n",
    "        for mask, (low, high) in zip(masks, ranges):\n",
    "            scores[mask] = np.random.randint(low, high, size=np.sum(mask))\n",
    "        \n",
    "        scores = np.where(anomalies == -1, scores * 0.7, scores)\n",
    "        scores = np.clip(scores, 0, 100)\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        components = pca.fit_transform(X)\n",
    "        \n",
    "        cluster_stats = pd.DataFrame({\n",
    "            'cluster': clusters,\n",
    "            'score': scores,\n",
    "            'volume': features['total_volume'].values,\n",
    "            'stability': features['stability'].values\n",
    "        }).groupby('cluster').agg({\n",
    "            'score': ['mean', 'std'],\n",
    "            'volume': 'mean',\n",
    "            'stability': 'mean',\n",
    "            'cluster': 'count'\n",
    "        })\n",
    "        cluster_stats.columns = ['mean_score', 'score_std', 'mean_volume', 'mean_stability', 'wallet_count']\n",
    "        \n",
    "        score_distribution = pd.Series(scores).value_counts().to_dict()\n",
    "        \n",
    "        scores = pd.Series(scores, index=features.index)\n",
    "        clusters = pd.Series(clusters, index=features.index)\n",
    "        \n",
    "        return scores, cluster_stats, components, score_distribution\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Scoring failed: {str(e)}\")\n",
    "        \n",
    "\n",
    "# Data Visualization Functions are over here to make clustering and other graphs used in main()\n",
    "def plot_cluster_analysis(features, clusters, components):\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # PCA Plot\n",
    "    plt.subplot(231)\n",
    "    sns.scatterplot(\n",
    "        x=components[:, 0],\n",
    "        y=components[:, 1],\n",
    "        hue=clusters,\n",
    "        palette='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title('Wallet Clusters in PCA Space')\n",
    "    \n",
    "    # Financial Profile\n",
    "    plt.subplot(232)\n",
    "    cluster_means = features.groupby(clusters)[['total_volume', 'stability', 'diversity']].mean()\n",
    "    cluster_means.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Cluster Financial Profiles')\n",
    "    \n",
    "    # Transaction Patterns\n",
    "    plt.subplot(233)\n",
    "    action_cols = [c for c in features.columns if c.startswith('count_')]\n",
    "    sns.heatmap(\n",
    "        features[action_cols].groupby(clusters).mean(),\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='YlGnBu'\n",
    "    )\n",
    "    plt.title('Transaction Patterns by Cluster')\n",
    "    \n",
    "    # Risk Analysis\n",
    "    plt.subplot(234)\n",
    "    risk_features = features[['liquidation_ratio', 'amount_std']].groupby(clusters).mean()\n",
    "    risk_features.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "    plt.title('Cluster Risk Profiles')\n",
    "    \n",
    "    # Stability vs Diversity\n",
    "    plt.subplot(235)\n",
    "    sns.scatterplot(\n",
    "        x=features['stability'],\n",
    "        y=features['diversity'],\n",
    "        hue=clusters,\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Stability vs Behavior Diversity')\n",
    "    \n",
    "    # Activity Duration\n",
    "    plt.subplot(236)\n",
    "    sns.boxplot(\n",
    "        x=clusters,\n",
    "        y=features['activity_days']\n",
    "    )\n",
    "    plt.title('Activity Duration by Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_score_analysis(scores, features):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Score Distribution\n",
    "    plt.subplot(231)\n",
    "    sns.histplot(scores, bins=20, kde=True)\n",
    "    plt.title('Credit Score Distribution')\n",
    "    \n",
    "    # Score vs Volume\n",
    "    plt.subplot(232)\n",
    "    sns.scatterplot(\n",
    "        x=np.log10(features['total_volume'] + 1),\n",
    "        y=scores,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    plt.title('Scores vs Transaction Volume (log)')\n",
    "    \n",
    "    # Score Components\n",
    "    plt.subplot(233)\n",
    "    score_factors = pd.DataFrame({\n",
    "        'Score': scores,\n",
    "        'Stability': features['stability'],\n",
    "        'Diversity': features['diversity'],\n",
    "        'Volume': np.log10(features['total_volume'] + 1)\n",
    "    })\n",
    "    sns.heatmap(\n",
    "        score_factors.corr(),\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0\n",
    "    )\n",
    "    plt.title('Score Component Correlations')\n",
    "    \n",
    "    # Risk Indicators\n",
    "    plt.subplot(234)\n",
    "    sns.boxplot(\n",
    "        x=pd.cut(scores, bins=5),\n",
    "        y=features['liquidation_ratio']\n",
    "    )\n",
    "    plt.title('Liquidation Ratio by Score Group')\n",
    "    \n",
    "    # Temporal Patterns\n",
    "    plt.subplot(235)\n",
    "    sns.scatterplot(\n",
    "        x=features['activity_days'],\n",
    "        y=scores,\n",
    "        hue=np.log10(features['txn_count'] + 1),\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Activity Duration vs Credit Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_grouped_score_counts(scores):\n",
    "    scores = np.array(scores)\n",
    "    total_wallets = len(scores)\n",
    "    \n",
    "    group_bins = [-1, 10, 25, 40, 65, 85, 100]\n",
    "    group_labels = [0, 1, 2, 3, 4, 5]\n",
    "    range_labels = [\"0-10\", \"11-25\", \"26-40\", \"41-65\", \"66-85\", \"86-100\"]\n",
    "    \n",
    "    precise_bins = [0, 10.0000001, 25.0000001, 40.0000001, 65.0000001, 85.0000001, 100.0000001]\n",
    "    \n",
    "    indices = np.digitize(scores, precise_bins, right=False) - 1\n",
    "    indices = np.clip(indices, 0, len(group_labels)-1)\n",
    "    \n",
    "    counts = np.bincount(indices, minlength=len(group_labels))\n",
    "    \n",
    "    group_df = pd.DataFrame({\n",
    "        'Group': group_labels,\n",
    "        'Score Range': range_labels,\n",
    "        'Wallet Count': counts,\n",
    "        'Percentage': (counts / total_wallets * 100).round(1)\n",
    "    })\n",
    "    \n",
    "    outliers = scores[(scores < 0) | (scores > 100.0000001)]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"Warning: {len(outliers)} outliers detected (min: {scores.min():.2f}, max: {scores.max():.2f})\")\n",
    "    \n",
    "    print(\"\\n=== Grouped Score Wallet Count ===\")\n",
    "    print(f\"Total Wallets: {total_wallets:,}\")\n",
    "    print(group_df[['Group', 'Score Range', 'Wallet Count', 'Percentage']].to_string(\n",
    "        index=False,\n",
    "        formatters={\n",
    "            'Wallet Count': '{:,.0f}'.format,\n",
    "            'Percentage': '{:.1f}%'.format\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    return len(outliers)\n",
    "\n",
    "def analyze_transaction_categories(transactions_df):\n",
    "    categories = {\n",
    "        'deposits': {'all_wallet_occurrences': 0, 'unique_wallets': set(), 'tx_counts': 0, 'total_volume': 0},\n",
    "        'withdraws': {'all_wallet_occurrences': 0, 'unique_wallets': set(), 'tx_counts': 0, 'total_volume': 0},\n",
    "        'borrows': {'all_wallet_occurrences': 0, 'unique_wallets': set(), 'tx_counts': 0, 'total_volume': 0},\n",
    "        'repays': {'all_wallet_occurrences': 0, 'unique_wallets': set(), 'tx_counts': 0, 'total_volume': 0},\n",
    "        'liquidates': {'all_wallet_occurrences': 0, 'unique_wallets': set(), 'tx_counts': 0, 'total_volume': 0}\n",
    "    }\n",
    "    \n",
    "    all_wallets = set()\n",
    "    total_wallet_occurrences = 0\n",
    "    \n",
    "    for _, row in transactions_df.iterrows():\n",
    "        action = row['action']\n",
    "        wallet = row['wallet']\n",
    "        amount = row['amountUSD']\n",
    "        \n",
    "        all_wallets.add(wallet)\n",
    "        total_wallet_occurrences += 1\n",
    "        \n",
    "        action_key = action\n",
    "        if action.endswith('s') and action[:-1] in categories:\n",
    "            action_key = action[:-1]\n",
    "        elif action + 's' in categories:\n",
    "            action_key = action + 's'\n",
    "        \n",
    "        if action_key in categories:\n",
    "            categories[action_key]['all_wallet_occurrences'] += 1\n",
    "            categories[action_key]['unique_wallets'].add(wallet)\n",
    "            categories[action_key]['tx_counts'] += 1\n",
    "            categories[action_key]['total_volume'] += amount\n",
    "    \n",
    "    categorized_wallets = set()\n",
    "    for cat in categories.values():\n",
    "        categorized_wallets.update(cat['unique_wallets'])\n",
    "    potential_missing = all_wallets - categorized_wallets\n",
    "    \n",
    "    stats = []\n",
    "    for cat_name, data in categories.items():\n",
    "        stats.append({\n",
    "            'Category': cat_name,\n",
    "            'Wallet Occurrences': data['all_wallet_occurrences'],\n",
    "            'Unique Wallets': len(data['unique_wallets']),\n",
    "            'Percentage of Total': f\"{(len(data['unique_wallets']) / len(all_wallets) * 100):.1f}%\",\n",
    "            'Transaction Count': data['tx_counts'],\n",
    "            'Total Volume (USD)': f\"${data['total_volume']:,.2f}\",\n",
    "            'Avg Volume per Wallet': f\"${(data['total_volume'] / len(data['unique_wallets'])):,.2f}\" if data['unique_wallets'] else \"N/A\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stats), potential_missing, total_wallet_occurrences\n",
    "\n",
    "def display_transaction_analysis(transactions_df):\n",
    "    stats_df, missing_wallets, total_wallet_occurrences = analyze_transaction_categories(transactions_df)\n",
    "    \n",
    "    print(\"\\n=== Transaction Category Analysis ===\")\n",
    "    print(f\"Total Wallet Transactions: {total_wallet_occurrences:,}\")\n",
    "    print(f\"Total Unique Wallets: {transactions_df['wallet'].nunique():,}\")\n",
    "    \n",
    "    print(\"\\nMain Categories:\")\n",
    "    print(stats_df[['Category', 'Wallet Occurrences', 'Unique Wallets', 'Percentage of Total', \n",
    "                    'Transaction Count', 'Total Volume (USD)', 'Avg Volume per Wallet']].to_string(index=False))\n",
    "    \n",
    "    if missing_wallets:\n",
    "        print(f\"\\nWarning: {len(missing_wallets):,} wallets had uncategorized transactions\")\n",
    "        print(\"Sample of affected wallets:\", list(missing_wallets)[:5])\n",
    "    \n",
    "    print(\"\\nAdditional Statistics:\")\n",
    "    deposits_count = int(stats_df[stats_df['Category'] == 'deposits']['Unique Wallets'])\n",
    "    liquidates_count = int(stats_df[stats_df['Category'] == 'liquidates']['Unique Wallets'])\n",
    "    most_active = stats_df.loc[stats_df['Transaction Count'].idxmax()]['Category']\n",
    "    \n",
    "    print(f\"- Wallets with only deposits: {deposits_count:,}\")\n",
    "    print(f\"- Wallets involved in liquidations: {liquidates_count:,}\")\n",
    "    print(f\"- Most active category: {most_active}\")\n",
    "\n",
    "# MAIN Function is over here\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"GPU config error: {e}\")\n",
    "    except:\n",
    "        print(\"GPU setting configuration failed.\")\n",
    "\n",
    "    try:\n",
    "        print(\"=== Zeru Finance Credit Scoring System ===\")\n",
    "        \n",
    "        print(\"\\n[1/4] Loading data...\")\n",
    "        data_dir = '/kaggle/input/eth-sample-dataset'\n",
    "        if not os.path.exists(data_dir):\n",
    "            print(f\"\\nError: Data directory not found at {data_dir}\")\n",
    "            print(\"Available directories in /kaggle/input:\")\n",
    "            print(os.listdir('/kaggle/input'))\n",
    "            return\n",
    "            \n",
    "        files = find_largest_json_files(data_dir)\n",
    "        print(f\"\\nProcessing files:\\n- \" + \"\\n- \".join(files))\n",
    "        \n",
    "        transactions = parse_transaction_data(files)\n",
    "        print(f\"\\nLoaded {len(transactions):,} transactions from {transactions['wallet'].nunique():,} unique wallets\")\n",
    "        \n",
    "        min_time = transactions['datetime'].min()\n",
    "        max_time = transactions['datetime'].max()\n",
    "        print(f\"\\nTime range of transactions: {min_time} to {max_time}\")\n",
    "        print(f\"Dataset covers {(max_time - min_time).days} days\")\n",
    "        \n",
    "        print(\"\\n[2/4] Creating behavioral features...\")\n",
    "        if len(transactions) == 0:\n",
    "            print(\"Error: No transactions to process\")\n",
    "            return\n",
    "    \n",
    "        features = create_wallet_features(transactions)\n",
    "        if features.empty:\n",
    "            print(\"Error: Feature engineering returned empty dataframe\")\n",
    "            return\n",
    "        \n",
    "        temporal_cols = [col for col in features.columns if col in [\n",
    "            'time_entropy', 'periodicity', 'time_skew', 'time_kurtosis',\n",
    "            'autocorr_1d', 'autocorr_7d', 'active_hours', 'time_std', 'recency'\n",
    "        ]]\n",
    "        print(f\"\\nGenerated {features.shape[1]} features per wallet, including temporal features:\")\n",
    "        print(features[temporal_cols].describe().transpose()[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "        print(\"\\n[3/4] Calculating credit scores...\")\n",
    "        scores, cluster_stats, components, score_distribution = compute_credit_scores(features)\n",
    "        features['credit_score'] = scores\n",
    "\n",
    "        print(\"\\nCalculating risk categories...\")\n",
    "        risk_bins = [0, 40, 70, 85, 100]\n",
    "        risk_labels = ['Low', 'Moderate', 'High', 'Very High']\n",
    "        \n",
    "        clipped_scores = np.clip(scores, 0, 100)\n",
    "        risk_categories = np.digitize(clipped_scores, risk_bins) - 1\n",
    "        valid_categories = np.clip(risk_categories, 0, len(risk_labels)-1)\n",
    "        features['risk_category'] = [risk_labels[x] for x in valid_categories]\n",
    "\n",
    "        print(\"\\n[4/4] Analyzing transaction patterns...\")\n",
    "        display_transaction_analysis(transactions)\n",
    "\n",
    "        print(\"\\n=== Score Distribution ===\")\n",
    "        missing = display_grouped_score_counts(scores)\n",
    "        \n",
    "        # Results Display Part\n",
    "        print(\"\\n=== Cluster Statistics ===\")\n",
    "        print(cluster_stats)\n",
    "        \n",
    "        print(\"\\n=== Score Distribution ===\")\n",
    "        score_dist = pd.Series(scores)\n",
    "        print(score_dist.describe())\n",
    "\n",
    "        print(\"\\n=== Wallet Statistics ===\")\n",
    "        score_counts = pd.Series(scores).value_counts().sort_index(ascending=False)\n",
    "        score_df = pd.DataFrame({\n",
    "            'Credit Score': score_counts.index,\n",
    "            'Wallet Count': score_counts.values,\n",
    "            'Percentage': (score_counts.values / len(scores) * 100).round(2)\n",
    "        })\n",
    "        \n",
    "        pd.set_option('display.max_rows', None)\n",
    "        print(\"\\nIndividual Credit Score Distribution:\")\n",
    "        print(score_df.to_string(index=False))\n",
    "        pd.reset_option('display.max_rows')\n",
    "\n",
    "        print(\"\\nGrouped Score Distribution:\")\n",
    "        display_grouped_score_counts(scores)\n",
    "        \n",
    "        risk_dist = pd.DataFrame({\n",
    "            'Risk Category': risk_labels,\n",
    "            'Wallet Count': np.bincount(valid_categories, minlength=4),\n",
    "            'Percentage': np.bincount(valid_categories, minlength=4) / len(valid_categories) * 100\n",
    "        })\n",
    "        print(\"\\n=== Risk Category Distribution ===\")\n",
    "        print(risk_dist.to_string(index=False, formatters={'Percentage': '{:.1f}%'.format}))\n",
    "        \n",
    "        print(\"\\n=== Temporal Patterns by Risk Category ===\")\n",
    "        temporal_stats = features.groupby('risk_category')[temporal_cols].mean()\n",
    "        print(temporal_stats.transpose())\n",
    "\n",
    "        \n",
    "        # Visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        plot_cluster_analysis(features, features['credit_score']//20, components)\n",
    "        plot_score_analysis(scores, features)\n",
    "\n",
    "        output = features[['credit_score']].sort_values('credit_score', ascending=False)\n",
    "        output_path = 'zeru_credit_scores.csv'\n",
    "        output.to_csv(output_path)\n",
    "\n",
    "        print(\"\\n=== Grouped Wallet Scores with Status ===\")\n",
    "        group_bins = [0, 10.0000001, 25.0000001, 40.0000001, 65.0000001, 85.0000001, 100.0000001]\n",
    "        range_labels = [\"0-10\", \"11-25\", \"26-40\", \"41-65\", \"66-85\", \"86-100\"]\n",
    "        group_labels = [0, 1, 2, 3, 4, 5]\n",
    "        \n",
    "        status_map = {\n",
    "            0: \"Highly Risky Behaviour - Immediate Investigation Necessary: Bot-Like or Exploitative behaviour\",\n",
    "            1: \"Highly Risky Behaviour - Immediate Investigation Necessary: Bot-Like or Exploitative behaviour\",\n",
    "            2: \"Risky Behaviour - Wallets need Investigation\",\n",
    "            3: \"Risky Behaviour - Wallets need Investigation\",\n",
    "            4: \"Mostly Safe Behaviour - Wallets Clear\",\n",
    "            5: \"Mostly Safe Behaviour - Wallets Clear\"\n",
    "        }\n",
    "        \n",
    "        clipped_scores = np.clip(scores, 0, 100)\n",
    "        indices = np.digitize(clipped_scores, group_bins, right=False) - 1\n",
    "        indices = np.clip(indices, 0, len(group_labels)-1)\n",
    "        \n",
    "        counts = np.bincount(indices, minlength=len(group_labels))\n",
    "        \n",
    "        status_df = pd.DataFrame({\n",
    "            'Group': group_labels,\n",
    "            'Score Range': range_labels,\n",
    "            'Wallet Count': counts,\n",
    "            'Percentage': (counts / len(scores) * 100).round(1),\n",
    "            'Status': [status_map[g] for g in group_labels]\n",
    "        })\n",
    "        \n",
    "        col_widths = {\n",
    "            'Group': 8,\n",
    "            'Score Range': 12,\n",
    "            'Wallet Count': 15,\n",
    "            'Percentage': 12,\n",
    "            'Status': 80  \n",
    "        }\n",
    "        formatted_df = status_df.copy()\n",
    "        formatted_df['Group'] = formatted_df['Group'].astype(str).str.ljust(col_widths['Group'])\n",
    "        formatted_df['Score Range'] = formatted_df['Score Range'].str.ljust(col_widths['Score Range'])\n",
    "        formatted_df['Wallet Count'] = formatted_df['Wallet Count'].apply(lambda x: f\"{x:,.0f}\".ljust(col_widths['Wallet Count']))\n",
    "        formatted_df['Percentage'] = formatted_df['Percentage'].apply(lambda x: f\"{x:.1f}%\".ljust(col_widths['Percentage']))\n",
    "        formatted_df['Status'] = formatted_df['Status'].str.ljust(col_widths['Status'])\n",
    "        \n",
    "        print(formatted_df.to_string(index=False, header=True, justify='left'))\n",
    "\n",
    "        # CODE BELOW displays top 1000 wallets,First 100 will be displayed, Press Enter in output cell to go to next 100\n",
    "        # To end display enter -1 in input box and press enter\n",
    "        top_1000 = output.head(1000).copy()\n",
    "        top_1000['rank'] = range(1, len(top_1000)+1)\n",
    "        \n",
    "        print(f\"\\n=== Top 1000 Wallets by Credit Score ===\")\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', 1000)\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "        \n",
    "        chunk_size = 100\n",
    "        for i in range(0, 1000, chunk_size):\n",
    "            chunk = top_1000.iloc[i:i+chunk_size]\n",
    "            print(f\"\\nRank {i+1}-{i+len(chunk)}:\")\n",
    "            print(chunk[['credit_score']].to_string())\n",
    "            \n",
    "            if i+chunk_size < 1000:\n",
    "                user_input = input(\"\\nPress Enter to continue or '-1' to exit...\")\n",
    "                if user_input.strip() == \"-1\":\n",
    "                    print(\"\\nDisplay terminated by user.\")\n",
    "                    break\n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        pd.reset_option('display.max_rows')\n",
    "        pd.reset_option('display.max_columns')\n",
    "        pd.reset_option('display.width')\n",
    "        pd.reset_option('display.max_colwidth')\n",
    "\n",
    "\n",
    "        print(f\"\\nResults saved to {output_path}\")\n",
    "        print(\"\\n=== Process completed successfully ===\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! ERROR: {str(e)}\")\n",
    "        print(\"\\nDebugging Info:\")\n",
    "        print(f\"Current directory: {os.getcwd()}\")\n",
    "        print(f\"Directory contents: {os.listdir('.')}\")\n",
    "        if os.path.exists('/kaggle'):\n",
    "            print(\"\\nKaggle environment detected\")\n",
    "            print(\"/kaggle/input contents:\", os.listdir('/kaggle/input'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12833041,
     "datasetId": 7738368,
     "sourceId": 12282505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 138.243398,
   "end_time": "2025-06-25T15:38:58.513262",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-25T15:36:40.269864",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
